filter(cran, !isna(cran$r_version))
filter(cran, !isna(cran$r_version))
filter(cran, !is.na(cran$r_version))
filter(cran, !is.na(r_version))
# arrange --> to order the rows of a dataset
cran2 <- select(cran, ip_id, store)
# arrange --> to order the rows of a dataset
cran2 <- select(cran, size:ip_id)
arrange(cran2, ip_id) #from small to large in ip_id
arrange(cran2, desc(ip_id)) #from large to small in ip_d
arrange(cran2, package, ip_id)
arrange(cran2, country, desc(r_version), ip_id)
cran3 <- select(cran, ip_id, package, size)
cran3
mutate(cran3, size_mb=size/2^20)
mutate(Cran3, size_mb=size/2^20, size_gb=size_mb/2^10)
mutate(cran3, size_mb=size/2^20, size_gb=size_mb/2^10)
mutate(cran3, correct_size=size-1000)
mutate(cran3, correct_size=size+1000)
##summarize
summarize(cran, avg_bytes=mean(size))
library(swirl)
swirl()
library(dplyr)
cran <-tbl_df(mydf)
rm("mydf")
cran
?group_by
#break dataset into groups of rows based on valueof other variables
#group data by package name
by_package <-group_by(cran, package)
by_package
#summarize by group
summarize(by_package, mean(size))
submit()
pack_sum
# top 1%
#we need to know value of 'count' that splits data into the top 1% and bottom 99%
quantile(pack_sum$count, probs =0.99)
top_counts <- filder(pack_sum, count>679)
top_counts <- filter(pack_sum, count>679)
top_counts
View(top_counts)
#sort
top_counts_sorted <- arrange(top_counts, desc(count))
View(top_counts_sorted)
quantile(pack_sum$unique, probs=0.99)
top_unique <- filter(pack_sum, unique>465)
View(top_unique)
top_unique_sorted <-arrange(top_unique, desc(unique))
View(top_unique_sorted)
submit()
submit()
submit()
View(result3)
submit()
submit()
View(cran)
rest()
reset()
swirl()
cran %>%
select(ip_id, country, package, size) %>%
print
submit()
submit()
submit()
submit()
submit()
submit()
library(tidyr)
students
?gather
##Tidyr (1) each variable forms a column (2) each observation forms a row (3) each type of observational units form a table
gather(students, sex, count, -grade)
students2
gather(students2, sex_class, column, -grade)
gather(students2, sex_class, count, -grade)
gather(students2, sex_class, count, -grade)
res <- gather(students2, sex_class, count, -grade)
res
?separate
separate(data=res, col=sex_class, into=c("sex", "class"))
submit()
students3
submit()
submit()
submit()
?gather
submit()
submit()
# Call gather() to gather the columns class1
# through class5 into a new variable called class.
# The 'key' should be class, and the 'value'
# should be grade.
#
# tidyr makes it easy to reference multiple adjacent
# columns with class1:class5, just like with sequences
# of numbers.
#
# Since each student is only enrolled in two of
# the five possible classes, there are lots of missing
# values (i.e. NAs). Use the argument na.rm = TRUE
# to omit these values from the final result.
#
# Remember that when you're using the %>% operator,
# the value to the left of it gets inserted as the
# first argument to the function on the right.
# Consult ?gather and/or ?chain if you get stuck.
#
students3 %>%
gather(class, grades, class1:class5,  na.rm= TRUE) %>%
print
submit()
submit()
submit()
submit()
submit()
submit()
# Consult ?gather and/or ?chain if you get stuck.
#
students3 %>%
gather(class, grades,  na.rm= TRUE, -name) %>%
print
submit()
library(swirl)
swirl()
library(statsr)
library(dplyr)
library(ggplot2)
data(nycflights)
#variables names
names(nycflights)
#check the structure of dataframe
str(nycflights)
sfo_feb_flights <- nycflights %>%
filter(dest =="SFO", month == 2)
summarise(sfo_feb_flights, n())
View(sfo_feb_flights)
View(sfo_feb_flights)
ggplot(sfo_feb_flights, aes(x=arr_delay)) + geom_histogram
ggplot(sfo_feb_flights, aes(x=arr_delay)) + geom_histogram()
calculate median and iqr for delay grouped by carrier
sfo_feb_flights %>%
group_by(carrier) %>%
summarise(mean_ad= mean(arr_delay), IQR_ad=IQR(arr_delay))
nycflights %>%
group_by(month) %>%
summarise(mean_dd=mean(dep_delay)) %>%
arrange(desc(mean_dd))
nycflights %>%
group_by(month) %>%
summarise(mean_dd=mean(dep_delay), median_dd=median(dep_delay)) %>%
arrange(desc(mean_dd))
ggplot(nycflights, aes(x = month, y = dep_delay)) +
geom_boxplot()
ggplot(nycflights, aes(x = factor(month), y = dep_delay)) +
geom_boxplot()
mutate(nycflights, dep_type=ifelse(depdelay<5,"on time", "delayed"))
#create var that is an identifer if delay or not
mutate(nycflights, dep_type=ifelse(dep_delay<5,"on time", "delayed"))
#create var that is an identifer if delay or not
nyc_flights <- mutate(nycflights, dep_type=ifelse(dep_delay<5,"on time", "delayed"))
nyc_flights %>%
group_by(origin) %>%
summarise(ot_dep_rate=sum(dep_type)/n()) %>%
arrange(desc(ot_dep_rate))
nyc_flights %>%
group_by(origin) %>%
summarise(ot_dep_rate=sum(dep_type == "on time")/n()) %>%
arrange(desc(ot_dep_rate))
ggplot(nyc_flights, aes(x=origin, fill=dep_type)) +geom_bar()
nyc_flights %>%
group_by(tail_num) %>%
mutate(avg_speed=distance/(air_time/60))
View(nyc_flights)
View(nyc_flights)
nyc_flights %>%
group_by(tailnum) %>%
mutate(avg_speed=distance/(air_time/60))
nyc_flights %>%
group_by(tailnum) %>%
mutate(avg_speed=distance/(air_time/60)) %>%
select(tailnum, avg_speed)
#new var average speed
nyc_flights %>%
group_by(tailnum) %>%
mutate(avg_speed=distance/(air_time/60)) %>%
select(tailnum, avg_speed) %>%
arrange(desc(avg_speed))
nyc_flights <- nyc_flights %>%
group_by(tailnum) %>%
mutate(avg_speed=distance/(air_time/60)) %>%
select(tailnum, avg_speed) %>%
arrange(desc(avg_speed))
nyc_flights <- nyc_flights %>%
group_by(tailnum) %>%
mutate(avg_speed=distance/(air_time/60)) %>%
select(tailnum, avg_speed) %>%
arrange(desc(avg_speed))
ggplot(nyc_flights, aes(x=avg_speed, y=distance)) + geom_point()
nyc_flights %>%
group_by(tailnum) %>%
mutate(avg_speed=distance/(air_time/60)) %>%
select(tailnum, avg_speed) %>%
arrange(desc(avg_speed))
nyc_flights %>%
group_by(tailnum) %>%
mutate(avg_speed=distance/(air_time/60)) %>%
select(tailnum, avg_speed) %>%
arrange(desc(avg_speed))
nyc_flights <- nyc_flights %>%
group_by(tailnum) %>%
mutate(avg_speed=distance/(air_time/60)) %>%
select(tailnum, avg_speed) %>%
arrange(desc(avg_speed))
nyc_flights <- nyc_flights %>%
group_by(tailnum) %>%
mutate(avg_speed=distance/(air_time/60)) %>%
select(tailnum, avg_speed) %>%
arrange(desc(avg_speed))
nyc_flights <- nyc_flights %>%
group_by(tailnum) %>%
mutate(avg_speed=distance/(air_time/60)) %>%
select(tailnum, avg_speed) %>%
arrange(desc(avg_speed))
nyc_flights <- nyc_flights %>%
group_by(tailnum) %>%
mutate(avg_speed=distance/(air_time/60)) %>%
select(tailnum, avg_speed) %>%
arrange(desc(avg_speed))
nyc_flights %>%
group_by(tailnum) %>%
mutate(avg_speed=distance/(air_time/60)) %>%
select(tailnum, avg_speed) %>%
arrange(desc(avg_speed))
library(statsr)
library(dplyr)
library(ggplot2)
data(nycflights)
#variables names
names(nycflights)
sfo_feb_flights <- nycflights %>%
filter(dest =="SFO", month == 2)
ggplot(sfo_feb_flights, aes(x=arr_delay)) + geom_histogram()
ggplot(sfo_feb_flights, aes(x=arr_delay)) + geom_histogram(binwidth=15)
ggplot(sfo_feb_flights, aes(x=arr_delay)) + geom_histogram(binwidth=5)
nyc_flights %>%
group_by(tailnum) %>%
mutate(avg_speed=distance/(air_time/60)) %>%
select(tailnum, avg_speed) %>%
arrange(desc(avg_speed))
nycflights %>%
group_by(tailnum) %>%
mutate(avg_speed=distance/(air_time/60)) %>%
select(tailnum, avg_speed) %>%
arrange(desc(avg_speed))
ggplot(nyc_flights, aes(x=avg_speed, y=distance)) + geom_point()
ggplot(nycflights, aes(x=avg_speed, y=distance)) + geom_point()
nycflights <- nycflights %>%
group_by(tailnum) %>%
mutate(avg_speed=distance/(air_time/60)) %>%
select(tailnum, avg_speed) %>%
arrange(desc(avg_speed))
ggplot(nycflights, aes(x=avg_speed, y=distance)) + geom_point()
library(statsr)
library(dplyr)
library(ggplot2)
data(nycflights)
nycflights <- nycflights %>%
group_by(tailnum) %>%
mutate(avg_speed=distance/(air_time/60)) %>%
arrange(desc(avg_speed))
ggplot(nycflights, aes(x=avg_speed, y=distance)) + geom_point()
ggplot(nycflights, aes(x=distance, y=avg_speed)) + geom_point()
View(nycflights)
nyc_flights <- mutate(nycflights, arr_type=ifelse(arr_delay<0,"on time", "delayed"))
nyc_flights <- mutate(nycflights, dep_type=ifelse(dep_delay<0,"on time", "delayed"))
summarise(nyc_flights, check=sum(dep_type == "delayed")/arr_type == "on time")
summarise(nyc_flights, check=sum(dep_type == "delayed")/sum(arr_type == "on time"))
summarise(nyc_flights, check=sum(dep_type == "delayed")/sum(arr_type == "on time"))
View(nyc_flights)
nyc_flights <- mutate(nycflights, dep_type=ifelse(dep_delay<0,"on time", "delayed"))
summarise(nyc_flights, check=sum(dep_type == "delayed")/sum(arr_type == "on time"))
View(nyc_flights)
View(nyc_flights)
nyc_flights <- mutate(nycflights, arr_type=ifelse(arr_delay<0,"on time", "delayed"))
nyc_flights <- mutate(nycflights, dep_type=ifelse(dep_delay<0,"on time", "delayed"))
summarise(nyc_flights, check=sum(dep_type == "delayed")/sum(arr_type == "on time"))
nyc_flights <- mutate(nycflights, arr_type=ifelse(arr_delay<0,"on time", "delayed"))
nyc_flights <- mutate(nycflights, dep_type=ifelse(dep_delay<0,"on time", "delayed"))
#create var that is an identifer if delay or not
nyc_flights <- mutate(nycflights, arr_type=ifelse(arr_delay<0,"on time", "delayed"),  mutate(nycflights, dep_type=ifelse(dep_delay<0,"on time", "delayed")))
sfo_feb_flights %>%
group_by(carrier) %>%
summarise(mean_ad= mean(arr_delay), IQR_ad=IQR(arr_delay))
nycflights %>%
group_by(month) %>%
summarise(mean_dd=mean(dep_delay), median_dd=median(dep_delay)) %>%
arrange(desc(mean_dd))
nycflights %>%
group_by(month) %>%
summarise(mean_dd=mean(dep_delay), median_dd=median(dep_delay)) %>%
arrange(desc(median_dd))
display(print["Hello world!"])
# Display hello world
print(["Hello world!"])
# Display hello world
print("Hello world!")
z <- c(pi, 205, 149, -2)
y <- c(z, 555, z)
y <- 2 * y + 760
my_sqrt <- sqrt(y - 1)
0/0
x <- c(1, 2, NA, 4)
2+x
age <- c(12, 28, 35, 27, NA, 25, 32, 45, 31, 23, NA, 34)
age[c(-5, -11)]
library(tidyverse)
papers <- as_tibble(read_csv("C:\Users\J-PAL SEA\Documents\GitHub\learning\micromaster\week 1\CitesforSara.csv"))
library(tidyverse)
papers <- as_tibble(read_csv("C:\Users\J-PAL SEA\Documents\GitHub\learning\micromaster\week 1\CitesforSara.csv"))
papers <- as_tibble(read_csv("C:/Users/J-PAL SEA/Documents/GitHub/learning/micromaster/week 1/CitesforSara.csv"))
View(papers)
select?
?select
papers_select<-(select, journal, year, cites, title, au1)
papers_select<- select(papers, journal, year, cites, title, au1)
View(papers_select)
View(papers_select)
filter(papers_select, cites>=100)
papers_select %>%
group_by(journal) %>%
show()
papers_select %>%
group_by(journal) %>%
summarise(n())
papers_select %>%
group_by(journal) %>%
summarise(n(cites))
papers_select %>%
group_by(journal) %>%
summarize(n(cites))
papers_select %>%
group_by(journal) %>%
summarise(n(cites))
papers_select %>%
group_by(journal) %>%
summarise(n(cites))
papers_select %>%
group_by(journal) %>%
summarise(n(cites))
papers_select %>%
group_by(journal) %>%
summarise(n())
summarise?
sd
?summarise
papers_select %>%
group_by(journal) %>%
summarise(sum(cites))
papers %>% summarise(n_distinct(au1))
?contains
papers_female <- papers %>%
select(contains("female"))
View(papers_female)
View(papers_female)
View(papers_select)
View(papers_female)
library(rvest)
## FINGER EXERCISE WEEK 2 ##
install.packages("rvest")
install.packages("rvest")
library("rvest")
edxsubjects <- read_html("https://www.edx.org/subjects")
subjectshtml<-html_nodes(edxsubjects, ".align-items-center")
subjecttext<-html_text(subjectshtml)
print(subjecttext)
rm(list = ls())
#Question 9&10
histogram(successes)
sucesses <- rm(list = ls())
sucesses <- rbinom(8, 1000, 0.2)
#Question 9&10
hist(successes)
successes <- rbinom(8, 1000, 0.2)
#Question 9&10
hist(successes)
rbinom()
?rbimnom
?rbinom
hist(sucesses)
successes <- rbinom(8, 1000, 0.2)
successes <- rbinom(8, 1000, 0.2)
successes <- rbinom(8, 1000, 0.2)
successes <- rbinom(8, 1000, 0.2)
successes <- rbinom(8, 1000, 0.2)
#Question 9&10
hist(successes)
#Question 14
binom_draws <- as_tibble(data.frame(successes))
estimated_pf <- binom_draws %>%
group_by(______) %>%
_____(n=n()) %>%
mutate(freq=n/sum(______))
successes <- rbinom(8, 10000, 0.2)
hist(successes)
successes <- rbinom(8, 1000, 0.2)
successes <- rbinom(8, 1000, 0.2)
#Question 9&10
hist(successes)
#Part A
a <- dbinom(10, 7, 0.65)
#Part A
dbinom(10, 7, 0.65)
2/7
#Part A
dbinom(7, 10, 0.65)
pbinom(7, 10, 0.65)
#Part B
pbinom(7, 10, 0.65)
#Part C
pbinom(6, 10, 0.65, lower.tail=FALSE)
#Part C
pbinom(6, 10, 0.65)
1 - 0.486173
#Part C
pbinom(6, 10, 0.65, lower.tail=FALSE)
library(tidyr)
binom_draws <- as_tibble(data.frame(successes))
View(binom_draws)
View(binom_draws)
binom_draws <- as_tibble(data.frame(successes))
estimated_pf <- binom_draws %>%
group_by(successes) %>%
summarize(n=n()) %>%
mutate(freq=n/sum(successes))
ggplot(estimated_pf, aes(x=successes, y=freq)) +
geom_col() +
ylab("Estimated Density")
binom_draws <- as_tibble(data.frame(successes))
estimated_pf <- binom_draws %>%
group_by(successes) %>%
summarize(n=n()) %>%
mutate(freq=n/sum(successes))
ggplot(estimated_pf, aes(x=successes, y=freq)) +
geom_col() +
ylab("Estimated Density")
binom_draws <- as_tibble(data.frame(successes))
estimated_pf <- binom_draws %>%
group_by(successes) %>%
summarize(n=n()) %>%
mutate(freq=n/sum(successes))
ggplot(estimated_pf, aes(x=successes, y=freq)) +
geom_col() +
ylab("Estimated Density")
binom_draws <- as_tibble(data.frame(successes))
estimated_pf <- binom_draws %>%
group_by(successes) %>%
summarize(n=n()) %>%
mutate(freq=n/sum(successes))
ggplot(estimated_pf, aes(x=successes, y=freq)) +
geom_col() +
ylab("Estimated Density")
library(ggplot2)
binom_draws <- as_tibble(data.frame(successes))
estimated_pf <- binom_draws %>%
group_by(successes) %>%
summarize(n=n()) %>%
mutate(freq=n/sum(successes))
ggplot(estimated_pf, aes(x=successes, y=freq)) +
geom_col() +
ylab("Estimated Density")
library(tidyr)
View(nyc_flights)
View(sfo_feb_flights)
setwd("~/GitHub/jagakpk/data")
## Purpose: Creating beautiful graphs azek
## Dont forget to set your directory into Jaga Github folder!
# Load relevant libraries
library(ggplot2)
library(tidyverse)
library(cowplot)
# Load the stata dta
district_level <- as_tibble(read.dta("/build/output/cleaned-jaga-districtlevel-foranalysis.dta"))
install.packages("foreign")
# Load relevant libraries
library(ggplot2)
library(tidyverse)
library(cowplot)
library(foreign)
# Load the stata dta
district_level <- as_tibble(read.dta("/build/output/cleaned-jaga-districtlevel-foranalysis.dta"))
district_level <- as_tibble(read.dta("\build\output\cleaned-jaga-districtlevel-foranalysis.dta"))
# Load the stata dta
district_level <- as_tibble(read.dta("\\build\\output\\cleaned-jaga-districtlevel-foranalysis.dta"))
# Load the stata dta
district_level <- as_tibble(read.dta("\\build\\output\\cleaned-jaga-districtlevel-foranalysis.dta"))
# Load the stata dta
district_level <- as_tibble(read.dta("\\build\\output\\cleaned-jaga-districtlevel-foranalysis.dta"))
# Load the stata dta
district_level <- as_tibble(read.dta("\\build\\output\\cleaned-jaga-districtlevel-foranalysis.dta"))
# Load the stata dta
district_level <- as_tibble(read.dta("\\build\\output\\cleaned-jaga-districtlevel-foranalysis.dta"))
# Load the stata dta
district_level <- as_tibble(read.dta("~/build/output/cleaned-jaga-districtlevel-foranalysis.dta"))
# Load the stata dta
district_level <- as_tibble(read.dta("build/output/cleaned-jaga-districtlevel-foranalysis.dta"))
# Load the stata dta
district_level <- as_tibble(read.dta("build/output/cleaned-jaga-districtlevel-foranalysis.dta"))
install.packages("haven")
install.packages("haven")
